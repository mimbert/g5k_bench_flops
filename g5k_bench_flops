#!/usr/bin/env python

import sys, os, time, math, pprint
from os.path import join as pjoin
from execo import Remote, SequentialActions, Put, Get, Report, Process, filter_bad_hosts
from execo_g5k import get_host_attributes, OarSubmission, g5k_charter_time, config
from execo_engine import logger, ParamSweeper, sweep, slugify
from g5k_cluster_engine import g5k_cluster_engine, worker_log
from common import *

class g5k_bench_flops(g5k_cluster_engine):

    def __init__(self):
        super(g5k_bench_flops, self).__init__()
        self.options_parser.set_usage("usage: %prog <comma separated list of clusters>")
        self.options_parser.set_description("compile and install openmpi, atlas, xhpl, then run xhpl bench to get max flops for cluster nodes")
        self.options_parser.add_option("-o", dest = "oar_options", default = None,
                                       help = "oar reservation options")
        self.options_parser.add_option("-w", dest = "walltime", default = "5:0:0",
                                       help = "walltime of bench jobs", type = "string")
        self.options_parser.add_option("-n", dest = "num_replicas", type = "int", default = 5,
                                       help = "num xp replicas: how many repetition of bench runs")
        self.options_parser.add_option("-C", dest = "charter", action = "store_true", default = False,
                                       help = "activate submission of best-effort jobs during time periods where g5k charter is applicable")
        self.options_parser.add_option("-T", dest = "testing", action = "store_true", default = False,
                                       help = "use api branch testing (conflicts with option -C)")
        self.options_parser.add_option("-m", dest = "ramlimit", type = "int", default = 16,
                                       help = "ram limit in GB (to avoid bench taking too much time)")
        self.options_parser.add_argument("clusters", "comma separated list of clusters")
        self.prepare_path = pjoin(self.engine_dir, "preparation")

    def init(self):
        if len(self.args) != 1:
            print "ERROR: wrong number of arguments"
            self.options_parser.print_help(file=sys.stderr)
            exit(1)
        if self.options.testing:
            config.g5k_configuration['api_additional_args'].append("branch=testing")
        clusters = self.args[0].split(",")
        parameters = {
            "cluster": {},
            "blas": ["atlas"],
            "num_nodes": [1],
            "xhpl_nb": [ 64, 128, 256 ],
            "repl": range(0, self.options.num_replicas)
            }
        for cluster in clusters:
            attrs = get_host_attributes(cluster + "-1")
            num_cores = attrs["architecture"]["nb_cores"]
            free_mem = int(min(attrs["main_memory"]["ram_size"] - .3e9, self.options.ramlimit * 1e9))
            big_size = int(math.sqrt(free_mem/8.0)*0.8)
            parameters["cluster"][cluster] = {
                "num_cores": {
                    1: {
                        "xhpl_grid": [ (1, 1) ],
                        "xhpl_n": [big_size // num_cores],
                        },
                    num_cores: {
                        "xhpl_grid": [ (int(num_cores / p), p) for p in range(1, int(math.sqrt(num_cores)) + 1) ],
                        "xhpl_n": [big_size],
                        }
                    },
                "xhpl_pfact": [0, 1, 2],
                "xhpl_rfact": [0, 1, 2],
                }
        self.sweeper = ParamSweeper(pjoin(self.result_dir, "bench_params"),
                                    sweep(parameters),
                                    save_sweeps = True)
        logger.info("parameters:\n" + pprint.pformat(parameters))
        logger.info("len(sweeps) = %i" % len(sweep(parameters)))

    def get_clusters(self):
        return list(set([ comb["cluster"] for comb in self.sweeper.get_remaining() ]))

    def get_job(self, cluster):
        comb = self.sweeper.get_next(filtr = lambda r: filter(lambda comb: comb["cluster"] == cluster, r))
        if not comb:
            return None
        submission = OarSubmission(resources = "{cluster='%s'}/nodes=%i" % (cluster, comb['num_nodes']),
                                   walltime = self.options.walltime,
                                   name = "flopsworker",
                                   job_type = ["allow_classic_ssh"],
                                   additional_options = self.options.oar_options)
        if self.options.charter and g5k_charter_time(time.time()):
            submission.job_type.append("besteffort")
        return submission, comb

    def worker(self, cluster, site, comb, nodes, worker_index, oarsubmission, jobid):

        def check_enough_nodes(action, comb, message):
            if not action.ok:
                if action.stats()['num_ok'] < comb['num_nodes']:
                    worker_log.info("%s:\n%s" % (message, Report([action]).to_string()))
                    self.sweeper.cancel(comb)
                    return False
            return True

        if nodes == None:
            self.sweeper.cancel(comb)
            return
        comb_dir = pjoin(self.result_dir, slugify(comb))
        try:
            os.makedirs(comb_dir)
        except OSError:
            pass # if directory already exists (from a previously interrupted run)
        worker_log.info("generate bench params in %s" % (comb_dir,))
        xhplconf = """HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
8            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
{ns}         Ns
1            # of NBs
{nbs}        NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
{grid_ps}    Ps
{grid_qs}    Qs
16.0         threshold
1            # of panel fact
{pfacts}     PFACTs (0=left, 1=Crout, 2=Right)
2            # of recursive stopping criterium
2 4          NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
{rfacts}     RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
0            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
0            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
"""
        xhplconf = xhplconf.format(
            ns = comb["xhpl_n"],
            nbs = comb["xhpl_nb"],
            grid_ps = comb["xhpl_grid"][0],
            grid_qs = comb["xhpl_grid"][1],
            pfacts = comb["xhpl_pfact"],
            rfacts = comb["xhpl_rfact"])
        with open(pjoin(comb_dir,"HPL.dat"), "w") as f:
            print >> f, xhplconf
        # prepare nodes
        worker_log.info("copy files to nodes")
        preparation = SequentialActions([
                Remote("mkdir -p " + node_working_dir,
                       nodes),
                Put(
                    nodes,
                    local_files = ([ pjoin(self.prepare_path, prepared_archive(package, cluster))
                                     for package in [ "atlas", "openmpi", "hpl" ] ] +
                                   [ pjoin(self.engine_dir, "node_bench_flops"),
                                     pjoin(comb_dir, "HPL.dat") ]),
                    remote_location = node_working_dir)])
        preparation.run()
        if not check_enough_nodes(preparation, comb, "aborting, copy of files failed on too much nodes"): return
        nodes = filter_bad_hosts(preparation, nodes)
        # run bench
        worker_log.info("run bench on nodes")
        bench = Remote(
            "cd %s ; ./node_bench_flops %s %s %s %s > stdout" % (
                node_working_dir,
                comb["blas"],
                comb["num_cores"],
                packages["hpl"]["extract_dir"],
                " ".join([ prepared_archive(package, cluster)
                           for package in [ "atlas", "openmpi", "hpl" ] ])),
            nodes)
        bench.run()
        success = check_enough_nodes(bench, comb, "bench failed on too much nodes")
        # retrieve stdout from all node
        Process("mkdir -p " + " ".join([pjoin(comb_dir, n.address) for n in nodes])).run()
        worker_log.info("retrieve logs in %s" % (comb_dir,))
        retrieval1 = Get(
            nodes,
            remote_files = [
                "%s/%s/bin/Linux_PII_CBLAS/HPL.dat" % (node_working_dir, packages["hpl"]["extract_dir"]),
                "%s/stdout" % node_working_dir],
            local_location = pjoin(comb_dir, "{{{host}}}"))
        retrieval1.run()
        if not success:
            worker_log.info("bench failed - tried to get the logs - aborting")
            return
        # retrieve results
        nodes = filter_bad_hosts(bench, nodes)
        worker_log.info("retrieve results in %s" % (comb_dir,))
        retrieval2 = Get(
            nodes,
            remote_files = [ "%s/%s/bin/Linux_PII_CBLAS/HPL.out" % (node_working_dir, packages["hpl"]["extract_dir"]) ],
            local_location = pjoin(comb_dir, "{{{host}}}"))
        retrieval2.run()
        if not check_enough_nodes(retrieval2, comb, "aborting, results retrieval failed on too much nodes"):
            [ os.unlink(p) for p in find_files(comb_dir, "-name", "HPL.out") ]
            return
        worker_log.info("finished combination %s" % (comb,))
        self.sweeper.done(comb)

if __name__ == "__main__":
    e = g5k_bench_flops()
    e.start()
